<!DOCTYPE html> 
<html lang="en"> 
    <head> 
        <meta charset="utf-8"> 
        <meta content="width=device-width, initial-scale=1.0" name="viewport"> 
        <title>Cityscapes</title>
        <meta content="" name="description"> 
        <meta content="" name="keywords"> 
        <!-- Favicons -->         
        <link href="assets/img/favicon.png" rel="icon"> 
        <link href="assets/img/apple-touch-icon.png" rel="apple-touch-icon"> 
        <!-- Google Fonts -->         
        <link href="static/css/dcf675f5f0d442a09ec5dad3352ca0da.css" rel="stylesheet"> 
        <!-- Vendor CSS Files -->         
        <link href="static/css/animate.min.css" rel="stylesheet"> 
        <link href="static/css/aos.css" rel="stylesheet"> 
        <link href="static/css/bootstrap.min.css" rel="stylesheet"> 
        <link href="static/css/bootstrap-icons.css" rel="stylesheet"> 
        <link href="static/css/boxicons.min.css" rel="stylesheet"> 
        <link href="static/css/glightbox.min.css" rel="stylesheet"> 
        <link href="static/css/swiper-bundle.min.css" rel="stylesheet"> 
        <!-- Template Main CSS File -->         
        <link href="static/css/style.css" rel="stylesheet"> 
        <!-- =======================================================
        * Template Name: Moderna - v4.2.0
        * Template URL: https://bootstrapmade.com/free-bootstrap-template-corporate-moderna/
        * Author: BootstrapMade.com
        * License: https://bootstrapmade.com/license/
        ======================================================== -->         
    </head>     
    <body>
        <!-- ======= Header ======= -->         
        <header id="header" class="fixed-top d-flex align-items-center header-transparent"> 
            <div class="container d-flex justify-content-between align-items-center">
                <div class="logo"> 
                    <h1 class="text-light"><a href="index.html"><span>Cityscapes</span></a></h1>
                    <!-- Uncomment below if you prefer to use an image logo -->                     
                    <!-- <a href="index.html"><img src="static/picture/logo.png" alt="" class="img-fluid"></a>-->                     
                </div>                 
                <nav id="navbar" class="navbar"> 
                    <ul> 
                        <li>
                            <a class="active" href="index.html">Home</a>
                        </li>                         
                        <li class="dropdown">
                            <a href="./index_1.html#par"><span>Documentation</span> <i class="bi bi-chevron-down"></i></a>
                            <ul> 
                                <li>
                                    <a href="./index_1.html#par">Dataset partition description</a>
                                </li>    
                                <li>
                                    <a href="./index_1.html#data_usage">Data statistics</a>
                                </li>                             
                                <li>
                                    <a href="./index_1.html#data_format">Data Format</a>
                                </li>                                 
                                <li>
                                    <a href="./index_1.html#data_annotation">Data Annotation</a>
                                </li>
                                
                            </ul>                             
                        </li>                         
                        <li class="dropdown">
                            <a href="./index_4.html"><span>Download</span> <i class="bi bi-chevron-down"></i></a>
                            <ul>
                                <li>
                                    <a href="./index_4.html#instructions">Instructions</a>
                                </li>
                                <li>
                                    <a href="./index_4.html#links">Links</a>
                                </li>                                 
                            </ul>                             
                        </li>                         
                        <li class="dropdown">
                            <a href="./index_6.html"><span>Benchmark</span> <i class="bi bi-chevron-down"></i></a> 
                            <ul> 
                                <li>
                                    <a href="./index_6.html">algorithm benchmark</a>
                                </li>                                 
                            </ul>                             
                        </li>                         
                                                
                    </ul>                     
                    <i class="bi bi-list mobile-nav-toggle"></i> 
                </nav>
                <!-- .navbar -->
            </div>             
        </header>
        <!-- End Header -->         
        <!-- ======= Hero Section ======= -->         
        <section id="hero" class="d-flex justify-cntent-center align-items-center"> 
            <div id="heroCarousel" class="container carousel carousel-fade" data-bs-ride="carousel" data-bs-interval="5000"> 
                <!-- Slide 1 -->                 
                <div class="carousel-item active"> 
                    <div class="carousel-container"> 
                        <h2 class="animate__animated animate__fadeInDown">The Cityscapes Dataset</h2>
                        <p class="animate__animated animate__fadeInUp">A novel road corner case dataset for object detection in autonomous driving which contains ~10000 carefully selected road driving scenes with high-quality bounding box annotation for 43 representative road object categories. 
                        </p>
                    </div>                     
                </div>                 
                <!-- Slide 2 -->                 
                <div class="carousel-item"> 
                    <div class="carousel-container"> 
                        <h2 class="animate__animated animate__fadeInDown">Real-World Object-Level Corner Cases</h2>
                        <p class="animate__animated animate__fadeInUp">Cityscapes is comprised of a large, diverse set of stereo video sequences recorded in streets from 50 different cities. 5000 of these images have high-quality pixel-level annotations; 20 000 additional images have coarse annotations to enable methods that leverage large volumes of weakly-labeled data.
                        </p>
                    </div>                     
                </div>                 
                <!-- Slide 3 -->                 
                <div class="carousel-item"> 
                    <div class="carousel-container"> 
                        <h2 class="animate__animated animate__fadeInDown">Reliable Object Detection</h2>
                        <p class="animate__animated animate__fadeInUp">
                        Previous state-of-the-art object detectors trained on large-scale autonomous driving datasets suffer from a significant performance drop on Cityscapes, suggesting that reliable object detection is still far from solved.
                        We hope Cityscapes can serve as a valuable resource to promote reliable object detection systems.
                        </p>
                    </div>                     
                </div>                 
                <a class="carousel-control-prev" href="#heroCarousel" role="button" data-bs-slide="prev"> <span class="carousel-control-prev-icon bx bx-chevron-left" aria-hidden="true"></span> </a> 
                <a class="carousel-control-next" href="#heroCarousel" role="button" data-bs-slide="next"> <span class="carousel-control-next-icon bx bx-chevron-right" aria-hidden="true"></span> </a> 
            </div>             
        </section>
        <!-- End Hero -->         
        <main id="main"> 
            <!-- ======= Services Section ======= -->             
            <!-- End Services Section -->             
            <!-- ======= Why Us Section ======= -->             
            <!-- End Why Us Section -->             
            <!-- ======= Features Section ======= -->             
            <section class="features"> 
                <div class="container">
                    <div class="row aos-init aos-animate" data-aos="fade-up"></div>                     
                    <h3><b>Dataset Overview</b></h3>
                    <p>
                        Background
                    </p>
                    <p> 
                        Cordts 
 
 propose the Cityscapes benchmark suite and a corresponding dataset, speciﬁcally tailored for autonomous driving in an urban environment and involving a much wider range of highly complex inner-city street scenes that were recorded in 50 different cities. Cityscapes signiﬁcantly exceed previous efforts in terms of size, annotation richness, and, more importantly, scene complexity and variability. We go beyond pixel-level semantic labeling by also considering instance-level semantic labeling in both our annotations and evaluation metrics. To facilitate research on 3D scene understanding, and provide depth information through stereo vision.

Below shows one example figure(RGB) in the dataset.
<p>
    <img src="./static/images/background.png">
</p>
                    </p>


                    <h3><b>Lifelong learning algorithm overview</b></h3>
                    <p>
                        Testing Open-set recognition in Curb-detection datasets
                    </p>
                    <span style="font-weight: bold;">About Open-set recognition</span>
                    <p> 
                        Traditional classiﬁers are deployed under a closed-set setting, with both training and test classes belonging to the same set. However, real-world applications probably face the input of unknown categories, and the model will recognize them as known ones. Under such circumstances, open-set recognition is proposed to maintain classiﬁcation performance on known classes and reject unknowns. The closed-set models make overconﬁdent predictions over familiar known class instances so that calibration and thresholding across categories become essential issues when extending to an open-set environment.

                        This test aims to reproduce the CVPR2021 paper "Learning placeholders for open-set recognition".
                        
                        See here for details

                    </p>


                    <span style="font-weight: bold;">About Curb-detection datasets</span>
                    <p> 
                        Two datasets, SYNTHIA, and cityscape, were selected for this project. Because SYNTHIA is often easier to obtain than the real urban road dataset as simulated by the simulator in a real research environment, it is treated as known task data for model pre-training, while the real urban landscape image acquisition requires more resources and is more difficult to obtain, so it is treated as unknown task data.

                    </p>



                    <span style="font-weight: bold;">Benchmark Setting</span>
                    <span>Key settings of the test environment to Open-set recognition are as follows:</span>
                    <pre style="
                    text-align: left;
                    white-space: pre;
                    display: block;
                    overflow-x: auto;
                    padding: 0.5em;
                    background: rgb(240, 240, 240);
                    color: rgb(68, 68, 68);
                  ">
                        <code> 
   benchmarkingjob:
      # job name of bechmarking; string type;
      name: "benchmarkingjob"
      # the url address of job workspace that will reserve the output of tests; string type;
      workspace: "/ianvs/lifelong_learning_bench/workspace"
    
      # the url address of test environment configuration file; string type;
      # the file format supports yaml/yml;
      testenv: "./examples/curb-detection/lifelong_learning_bench/testenv/testenv.yaml"
    
      # the configuration of test object
      test_object:
        # test type; string type;
        # currently the option of value is "algorithms",the others will be added in succession.
        type: "algorithms"
        # test algorithm configuration files; list type;
        algorithms:
          # algorithm name; string type;
          - name: "rfnet_lifelong_learning"
            # the url address of test algorithm configuration file; string type;
            # the file format supports yaml/yml
            url: "./examples/curb-detection/lifelong_learning_bench/testalgorithms/rfnet/rfnet_algorithm.yaml"
                        </code>
                    </pre>

                    <span>
                        Key settings of the algorithm are as follows:
                    </span>
                    <pre style="
                    text-align: left;
                    white-space: pre;
                    display: block;
                    overflow-x: auto;
                    padding: 0.5em;
                    background: rgb(240, 240, 240);
                    color: rgb(68, 68, 68);
                  ">
                        <code> 
    algorithm:
    # paradigm type; string type;
    # currently the options of value are as follows:
    #   1> "singletasklearning"
    #   2> "incrementallearning"
    #   3> "lifelonglearning"
    paradigm_type: "lifelonglearning"
    lifelong_learning_data_setting:
        # ratio of training dataset; float type;
        # the default value is 0.8.
        train_ratio: 0.8
        # the method of splitting dataset; string type; optional;
        # currently the options of value are as follows:
        #   1> "default": the dataset is evenly divided based train_ratio;
        splitting_method: "default"

    # algorithm module configuration in the paradigm; list type;
    modules:
        # type of algorithm module; string type;
        # currently the options of value are as follows:
        #   1> "basemodel": contains important interfaces such as train、 eval、 predict and more; required module;
        - type: "basemodel"
        # name of python module; string type;
        # example: basemodel.py has BaseModel module that the alias is "FPN" for this benchmarking;
        name: "BaseModel"
        # the url address of python module; string type;
        url: "./examples/curb-detection/lifelong_learning_bench/testalgorithms/rfnet/basemodel.py"
        # hyperparameters configuration for the python module; list type;
        hyperparameters:
            # name of the hyperparameter; string type;
            - learning_rate:
                values:
                - 0.0001
        #  2> "task_definition": define lifelong task ; optional module;
        - type: "task_definition"
        # name of python module; string type;
        name: "TaskDefinitionByOrigin"
        # the url address of python module; string type;
        url: "./examples/curb-detection/lifelong_learning_bench/testalgorithms/rfnet/task_definition_by_origin.py"
        # hyperparameters configuration for the python module; list type;
        hyperparameters:
            # name of the hyperparameter; string type;
            # origins of data; value is ["real", "sim"], this means that data from real camera and simulator.
            - origins:
                values:
                - [ "real", "sim" ]
        #  3> "task_allocation": allocate lifelong task ; optional module;
        - type: "task_allocation"
        # name of python module; string type;
        name: "TaskAllocationByOrigin"
        # the url address of python module; string type;
        url: "./examples/curb-detection/lifelong_learning_bench/testalgorithms/rfnet/task_allocation_by_origin.py"
        # hyperparameters configuration for the python module; list type;
        hyperparameters:
            # name of the hyperparameter; string type;
            # origins of data; value is ["real", "sim"], this means that data from real camera and simulator.
            - origins:
                values:
                - [ "real", "sim" ]
        - type: "unknow_task_recognition"
        # name of python module; string type;
        name: "SampleRegonitionByScene"
        # the url address of python module; string type;
        url: "./examples/curb-detection/lifelong_learning_bench/testalgorithms/rfnet/unknow_task_recognition.py"
            # hyperparameters configuration for the python module; list type;
        hyperparameters:
            -model_path:
            values:
                - "/examples/curb-detection/lifelong_learning_bench/testalgorithms/rfnet/results/Epochofprose17.pth"
                        </code>
                    </pre>
                
                    <span style="font-weight: bold;margin-top: 20px;display: inline-block;">About Curb-detection datasets</span>
                    <p> 
                        Two datasets, SYNTHIA, and cityscape, were selected for this project. Because SYNTHIA is often easier to obtain than the real urban road dataset as simulated by the simulator in a real research environment, it is treated as known task data for model pre-training, while the real urban landscape image acquisition requires more resources and is more difficult to obtain, so it is treated as unknown task data.

                    </p>
                   
                     <h3>Data sample display</h3>
                    <p>
                        <img src="./static/images/2.1.png" style="margin-bottom: 20px;">
                        
                        <img src="./static/images/2.2.png">
                    </p>
                     
                    <!-- <h3><b>Class Definitions</b></h3>
                    <span>Please click on the individual classes for details on their definitions.</span>
                     <p>
                        <table>
                            <tr>
                              <th>Group</th>
                              <th>Classes</th>
                            </tr>
                            <tr>
                              <td>flat</td>
                              <td>road · sidewalk · parking+ · rail track+</td>
                            </tr>
                             <tr>
                              <td>human</td>
                              <td>person* · rider*</td>
                             
                              </tr>
                            <tr>
                               <td>vehicle</td>
                              <td>car* · truck* · bus* · on rails* · motorcycle* · bicycle* · caravan*+ · trailer*+</td>
                             </tr>
                             <tr>
                                <td>construction</td>
                               <td>building · wall · fence · guard rail+ · bridge+ · tunnel+</td>
                              </tr>
                              <tr>
                                <td>object</td>
                               <td>	pole · pole group+ · traffic sign · traffic light</td>
                              </tr>
                              <tr>
                                <td>nature</td>
                               <td>vegetation · terrain</td>
                              </tr>
                              <tr>
                                <td>sky</td>
                               <td>sky</td>
                              </tr>
                              <tr>
                                <td>void</td>
                               <td>ground+ · dynamic+ · static+</td>
                              </tr>
                         </table>
                    </p>  
                    * Single instance annotations are available. However, if the boundary between such instances cannot be clearly seen, the whole crowd/group is labeled together and annotated as group, e.g. car group.
                    + This label is not included in any evaluation and treated as void (or in the case of license plate as the vehicle mounted on). -->
                    <div class="col-md-7 order-2 order-md-1 pt-5">
                    </div>
                    
                    </div>
                    <!-- <h3><b>Citation</b></h3>
                    <pre style="text-align: left; white-space: pre; display: block; overflow-x: auto; padding: 0.5em; background: rgb(240, 240, 240); color: rgb(68, 68, 68);"> TBD </pre> -->
                </div>                 
            </section>
            <!-- End Features Section -->             
        </main>
        <!-- End #main -->         
        <!-- ======= Footer ======= -->         
        <footer id="footer" data-aos-easing="ease-in-out" data-aos-duration="500" class="aos-init"> 
            <div class="container"> 
                <div class="copyright"> 
                    &copy; Copyright 
                    <strong><span>HUAWEI</span></strong>. All Rights Reserved
                </div>                 
                <div class="credits"> 
                    <!-- All the links in the footer should remain intact. -->                     
                    <!-- You can delete the links only if you purchased the pro version. -->                     
                    <!-- Licensing information: https://bootstrapmade.com/license/ -->                     
                    <!-- Purchase the pro version with working PHP/AJAX contact form: https://bootstrapmade.com/free-bootstrap-template-corporate-moderna/ -->                     
                    Designed by 
                    <a href="https://bootstrapmade.com/">BootstrapMade</a> 
                </div>                 
            </div>             
        </footer>
        <!-- End Footer -->         
        <a href="#" class="back-to-top d-flex align-items-center justify-content-center"><i class="bi bi-arrow-up-short"></i></a> 
        <!-- Vendor JS Files -->         
        <script src="static/js/aos.js"></script>         
        <script src="static/js/bootstrap.bundle.min.js"></script>         
        <script src="static/js/glightbox.min.js"></script>         
        <script src="static/js/isotope.pkgd.min.js"></script>         
        <script src="static/js/validate.js"></script>         
        <script src="static/js/purecounter.js"></script>         
        <script src="static/js/swiper-bundle.min.js"></script>         
        <script src="static/js/noframework.waypoints.js"></script>         
        <!-- Template Main JS File -->         
        <script src="static/js/main.js"></script>
    </body>     
</html>
